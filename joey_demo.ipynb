{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "joey-demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYu2nX+v4JAG63Zh73F6y0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joeynmt/joeynmt/blob/master/joey_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjX7C7AHX-8F"
      },
      "source": [
        "# Joey NMT Demo\n",
        "\n",
        "In this notebook, we'll train a Transformer model for translating between simple sentences in Esperanto (*epo*) and English (*eng*). You'll have the option to choose your own languages as well.\n",
        "\n",
        "**Important:** Before you start, set runtime type to GPU.\n",
        "\n",
        "Author: Julia Kreutzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYRtQZFPYzES"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4VofGiF2MMr"
      },
      "source": [
        "Install the right PyTorch version for Joey NMT. Might have to restart the colab after installing Joey NMT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG-XRA4tX2oV",
        "outputId": "3d9ab254-9447-4920-d3a1-3d44b9582763"
      },
      "source": [
        "!pip install torch==1.7.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 25kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.7.1+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed torch-1.7.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a4u741jiY2-O",
        "outputId": "4ea5af54-10b4-4bca-a303-c68d597e3b0f"
      },
      "source": [
        "!pip install joeynmt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting joeynmt\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/28/9e2df8769162c911955015a381ea76f4a7248d0fe2169a4ed8b7b5193cd6/joeynmt-1.2-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 21.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 20.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 71kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 8.1MB/s \n",
            "\u001b[?25hCollecting torchtext==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/80/046f0691b296e755ae884df3ca98033cb9afcaf287603b2b7999e94640b8/torchtext-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (2.4.1)\n",
            "Collecting six==1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt) (7.0.0)\n",
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (1.7.1+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt) (3.2.2)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 51.0MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/9f/5986adadc51c867799bc076431fba64fd8ec45d2e528f974838df8aca9da/pylint-2.7.1-py3-none-any.whl (343kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (53.0.0)\n",
            "Collecting numpy<1.19.0,>=1.14.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 173kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.16.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.11.1)\n",
            "Collecting sacrebleu>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.5MB/s \n",
            "\u001b[?25hCollecting wrapt==1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1->joeynmt) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1->joeynmt) (4.41.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.27.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->joeynmt) (3.7.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.4.7)\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt) (0.10.2)\n",
            "Collecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting astroid==2.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/f0/2364d469327ffef8ee1964a5995f8206fd22fcfa57f2618498f8b963329f/astroid-2.5-py3-none-any.whl (220kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 57.7MB/s \n",
            "\u001b[?25hCollecting isort<6,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/89/6888f573886e9dc0906ec98f1b15888de20919a142c355d7f57ebd977d36/isort-5.7.0-py3-none-any.whl (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.4.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/82/22/e684c9e2e59b561dbe36538852e81849122c666c423448e3a5c99362c228/portalocker-2.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1->joeynmt) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1->joeynmt) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1->joeynmt) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1->joeynmt) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.7.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt) (3.4.0)\n",
            "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/76/04c8d23cf9da13b6c892055148cdabb79d8d835dd816d09d529c1c615b20/typed_ast-1.4.2-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 49.5MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/2b/a6949413f7c74535b2cb21741376d44f985bbf340230b31a1a6030da0108/lazy_object_proxy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt) (2018.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->joeynmt) (3.4.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68373 sha256=3cf09b5cbab5b588e0898f0ce278197fc1f5d9bb0dd372040f926bb9497243e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built wrapt\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.7.1+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement wrapt~=1.12.1, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: nbclient 0.5.2 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, torchtext, six, pyyaml, mccabe, typed-ast, lazy-object-proxy, wrapt, astroid, isort, pylint, portalocker, sacrebleu, subword-nmt, joeynmt\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "Successfully installed astroid-2.5 isort-5.7.0 joeynmt-1.2 lazy-object-proxy-1.5.2 mccabe-0.6.1 numpy-1.18.5 portalocker-2.2.1 pylint-2.7.1 pyyaml-5.4.1 sacrebleu-1.5.0 six-1.12.0 subword-nmt-0.3.7 torchtext-0.8.1 typed-ast-1.4.2 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrcFeSdabKHw"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "We'll use English - Esperanto translations from the Tatoeba challenge. \n",
        "\n",
        "If you want to use a different language pair, you'll need to replace all instances of `eng` and `epo` language identifiers to your languages of choice. You can find all available languages [here](https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/Data.md). Note that if the training data size is larger, data preparation and training might take much longer than the example with only 400k sentence pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF1998yPmtbS"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6cux99eZ-gW",
        "outputId": "18fa0b4c-c12d-434b-ae5c-21fcb5d73f8d"
      },
      "source": [
        "!wget https://object.pouta.csc.fi/Tatoeba-Challenge/eng-epo.tar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-24 07:48:53--  https://object.pouta.csc.fi/Tatoeba-Challenge/eng-epo.tar\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35747840 (34M) [application/x-tar]\n",
            "Saving to: ‘eng-epo.tar’\n",
            "\n",
            "eng-epo.tar         100%[===================>]  34.09M  16.5MB/s    in 2.1s    \n",
            "\n",
            "2021-02-24 07:48:56 (16.5 MB/s) - ‘eng-epo.tar’ saved [35747840/35747840]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_kVvMcBbgLg",
        "outputId": "19b077bd-a441-40a5-ed36-0eb878d6ff0d"
      },
      "source": [
        "!tar -xvf  'eng-epo.tar'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/eng-epo/\n",
            "data/eng-epo/train.src.gz\n",
            "data/eng-epo/dev.trg\n",
            "data/eng-epo/train.id.gz\n",
            "data/eng-epo/test.trg\n",
            "data/eng-epo/test.id\n",
            "data/eng-epo/dev.src\n",
            "data/eng-epo/dev.id\n",
            "data/eng-epo/test.src\n",
            "data/eng-epo/train.trg.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeIGVD2rcfFD"
      },
      "source": [
        "!gunzip 'data/eng-epo/train.src.gz'\n",
        "!gunzip 'data/eng-epo/train.trg.gz'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBVyCK9aiPcE"
      },
      "source": [
        "We'll only use a subset of dev and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHm_0HlIeshK"
      },
      "source": [
        "!mv 'data/eng-epo/train.src' 'data/eng-epo/train.eng'\n",
        "!head -n 1000 'data/eng-epo/dev.src' > 'data/eng-epo/dev.eng'\n",
        "!head -n 1000 'data/eng-epo/test.src' > 'data/eng-epo/test.eng'\n",
        "!mv 'data/eng-epo/train.trg' 'data/eng-epo/train.epo'\n",
        "!head -n 1000 'data/eng-epo/dev.trg' > 'data/eng-epo/dev.epo'\n",
        "!head -n 1000 'data/eng-epo/test.trg' > 'data/eng-epo/test.epo'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoveftbNCyv8"
      },
      "source": [
        "The data is sentence-aligned, that means that source and target file contain one sentence per line which correspond to each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLNgutcOcHK2",
        "outputId": "37edd487-2391-4fc5-de7c-60dd2dead944"
      },
      "source": [
        "! head data/eng-epo/dev.eng\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I made you something.\n",
            "Twice two is four.\n",
            "That hurts! Stop it!\n",
            "It is too much for me. I need to slow down.\n",
            "I never want to see him again.\n",
            "At what hour was she born?\n",
            "The traffic jam lasted one hour.\n",
            "After dinner, we played cards till eleven.\n",
            "I doubt if he is a lawyer.\n",
            "Who's on duty today?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrP5Ujk0cXxA",
        "outputId": "ac281255-0b1c-4d1d-cea7-4d3c02139ba4"
      },
      "source": [
        "! head data/eng-epo/dev.epo"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mi faris ion por vi.\n",
            "Du oble du faras kvar.\n",
            "Tio suferigas min! Ĉesu!\n",
            "Estas tro multe por mi. Mi devas malrapidiĝi.\n",
            "Mi volas neniam plu vidi lin.\n",
            "Je kioma horo ŝi naskiĝis?\n",
            "La trafikmalfluo daŭris unu horon.\n",
            "Post la vespermanĝo ni kartludis ĝis la dudek tria.\n",
            "Mi dubas ĉu li estas advokato.\n",
            "Kiu deĵoras hodiaŭ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydKB8OS1c0r1",
        "outputId": "7d5c06a9-4822-4c35-af2a-621311d3e0bc"
      },
      "source": [
        "! wc -l data/eng-epo/*"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    1000 data/eng-epo/dev.eng\n",
            "    1000 data/eng-epo/dev.epo\n",
            "  235834 data/eng-epo/dev.id\n",
            "  235834 data/eng-epo/dev.src\n",
            "  235834 data/eng-epo/dev.trg\n",
            "    1000 data/eng-epo/test.eng\n",
            "    1000 data/eng-epo/test.epo\n",
            "   10000 data/eng-epo/test.id\n",
            "   10000 data/eng-epo/test.src\n",
            "   10000 data/eng-epo/test.trg\n",
            "  402180 data/eng-epo/train.eng\n",
            "  402180 data/eng-epo/train.epo\n",
            "    1389 data/eng-epo/train.id.gz\n",
            " 1547251 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZg4k1rem1jI"
      },
      "source": [
        "## Subword model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJl9sQq22c5Z"
      },
      "source": [
        "We will use the `subword_nmt` library to split words into subwords (BPE) according to their frequency in the training corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGDmK7rnqc6r"
      },
      "source": [
        "import os"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wNsekzFd1BD"
      },
      "source": [
        "src_lang = 'epo'\n",
        "trg_lang = 'eng'\n",
        "bpe_size = 4000\n",
        "datadir = '/content/data/eng-epo/'\n",
        "name = f'{src_lang}_{trg_lang}_bpe{bpe_size}'\n",
        "\n",
        "\n",
        "train_src_file = os.path.join(datadir, f'train.{src_lang}')\n",
        "train_trg_file = os.path.join(datadir, f'train.{trg_lang}')\n",
        "train_joint_file = os.path.join(datadir, f'train.{src_lang}-{trg_lang}')\n",
        "dev_src_file = os.path.join(datadir, f'dev.{src_lang}')\n",
        "dev_trg_file = os.path.join(datadir, f'dev.{trg_lang}')\n",
        "test_src_file = os.path.join(datadir, f'test.{src_lang}')\n",
        "test_trg_file = os.path.join(datadir, f'test.{trg_lang}')\n",
        "src_files = {'train': train_src_file, 'dev': dev_src_file, 'test': test_src_file}\n",
        "trg_files = {'train': train_trg_file, 'dev': dev_trg_file, 'test': test_trg_file}\n",
        "\n",
        "\n",
        "vocab_src_file = os.path.join(datadir, f'vocab.{bpe_size}.{src_lang}')\n",
        "vocab_trg_file = os.path.join(datadir, f'vocab.{bpe_size}.{trg_lang}')\n",
        "bpe_file = os.path.join(datadir, f'bpe.codes.{bpe_size}')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJPV_l_G2ny1"
      },
      "source": [
        "Train a BPE model with 4000 symbols for both languages jointly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVYh30Mjm3zu"
      },
      "source": [
        "! cat $train_src_file $train_trg_file > $train_joint_file\n",
        "\n",
        "! subword-nmt learn-bpe \\\n",
        "  --input $train_joint_file \\\n",
        "  -s $bpe_size \\\n",
        "  -o $bpe_file"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEQjXFqv2u-3"
      },
      "source": [
        "This file contains the merges of character sequences that subwords are made of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtUq1rg1sA8H",
        "outputId": "55c52416-7736-4a07-b092-a6de2a82682a"
      },
      "source": [
        "! head $bpe_file"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#version: 0.2\n",
            "t h\n",
            "a n\n",
            "o n\n",
            "e r\n",
            "i n\n",
            "e n\n",
            "s t\n",
            "l a</w>\n",
            "o r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWdUshWa2z3V"
      },
      "source": [
        "We apply the learned BPE merges to training, development and test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgdj1d3wpA1c"
      },
      "source": [
        "src_bpe_files = {}\n",
        "trg_bpe_files = {}\n",
        "for split in ['train', 'dev', 'test']:\n",
        "  src_input_file = src_files[split]\n",
        "  trg_input_file = trg_files[split]\n",
        "  src_output_file = src_input_file.replace(split, f'{split}.{bpe_size}.bpe')\n",
        "  trg_output_file = trg_input_file.replace(split, f'{split}.{bpe_size}.bpe')\n",
        "  src_bpe_files[split] = src_output_file\n",
        "  trg_bpe_files[split] = trg_output_file\n",
        "\n",
        "  ! subword-nmt apply-bpe \\\n",
        "    -c $bpe_file \\\n",
        "    < $src_input_file > $src_output_file\n",
        "\n",
        "  ! subword-nmt apply-bpe \\\n",
        "    -c $bpe_file \\\n",
        "    < $trg_input_file > $trg_output_file\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KrKE20C27z3"
      },
      "source": [
        "The subword-split data contains `@@ ` to indicate where words were split into subwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZvrbM5Qx418",
        "outputId": "1fc011fa-1996-492b-aa32-f80f6b34a7d9"
      },
      "source": [
        "! head data/eng-epo/dev.4000.bpe.eng"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I made you some@@ thing.\n",
            "T@@ w@@ ice two is f@@ our@@ .\n",
            "That h@@ ur@@ t@@ s! Stop it@@ !\n",
            "It is too much for me. I need to s@@ low dow@@ n.\n",
            "I never want to see him again@@ .\n",
            "A@@ t what h@@ our was she bor@@ n@@ ?\n",
            "The traf@@ fi@@ c jam last@@ ed one h@@ our@@ .\n",
            "Af@@ ter d@@ in@@ n@@ er, we played c@@ ards til@@ l el@@ ev@@ en.\n",
            "I d@@ ou@@ b@@ t if he is a law@@ y@@ er.\n",
            "Wh@@ o@@ 's on d@@ ut@@ y t@@ od@@ ay@@ ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3uCvTzuyEqo",
        "outputId": "766f94c1-6b3f-4454-b691-a840a1d9a31e"
      },
      "source": [
        "! head data/eng-epo/dev.4000.bpe.epo"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mi faris ion por vi.\n",
            "D@@ u o@@ ble du faras kvar@@ .\n",
            "Tio sufer@@ igas min@@ ! Ĉes@@ u!\n",
            "Estas tro multe por mi. Mi devas mal@@ rapi@@ di@@ ĝ@@ i.\n",
            "Mi volas neniam plu vidi lin.\n",
            "J@@ e ki@@ om@@ a h@@ oro ŝi n@@ aski@@ ĝ@@ is?\n",
            "La tra@@ fik@@ mal@@ fl@@ uo daŭ@@ ris unu hor@@ on.\n",
            "Post la vesper@@ man@@ ĝo ni kart@@ lud@@ is ĝis la dudek tri@@ a.\n",
            "Mi du@@ b@@ as ĉu li estas ad@@ vok@@ at@@ o.\n",
            "Kiu de@@ ĵ@@ or@@ as hodi@@ aŭ@@ ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jS8RHLZyKKf"
      },
      "source": [
        "## Prepare the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndkGOp7F3LWY"
      },
      "source": [
        "From the pre-processed training data, we extract the final vocabulary for the translation model. It should contain all subwords needed for representing the source and target training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGq8KMqjySXm",
        "outputId": "9d42be9a-8a47-450f-accc-996373f62d44"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-24 07:49:54--  https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2034 (2.0K) [text/plain]\n",
            "Saving to: ‘build_vocab.py’\n",
            "\n",
            "build_vocab.py      100%[===================>]   1.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-24 07:49:54 (52.8 MB/s) - ‘build_vocab.py’ saved [2034/2034]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1_iQEQEyJyS"
      },
      "source": [
        "vocab_src_file = src_bpe_files['train']\n",
        "vocab_trg_file = trg_bpe_files['train']\n",
        "bpe_vocab_file = os.path.join(datadir, f'joint.{bpe_size}bpe.vocab')\n",
        "\n",
        "! python build_vocab.py  \\\n",
        "  $vocab_src_file $vocab_trg_file \\\n",
        "  --output_path $bpe_vocab_file"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSuYid3JdECc"
      },
      "source": [
        "# Model configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45uGP83v3Y24"
      },
      "source": [
        "Joey NMT reads model and training hyperparameters from a configuration file. We're generating this now to configure paths in the appropriate places. \n",
        "\n",
        "The configuration below builds a small Transformer model with shared embeddings between source and target language on the base of the subword vocabularies created above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EirEmJmkc7sx"
      },
      "source": [
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"{datadir}/train.{bpe_size}.bpe\"\n",
        "    dev:   \"{datadir}/dev.{bpe_size}.bpe\"\n",
        "    test:  \"{datadir}/test.{bpe_size}.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False                \n",
        "    max_sent_length: 30             # Extend to longer sentences.\n",
        "    src_vocab: \"{vocab_src_file}\"\n",
        "    trg_vocab: \"{vocab_trg_file}\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "    sacrebleu:                      # sacrebleu options\n",
        "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
        "        tokenize: \"intl\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
        "\n",
        "training:\n",
        "    #load_model: \"models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # Alternative: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 2000          # Set to at least once per epoch.\n",
        "    logging_freq: 200\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: False               # Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True       # Requires joint vocabulary.\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # Increase to 512 for larger data.\n",
        "        ff_size: 1024            # Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, source_language=src_lang, target_language=trg_lang,\n",
        "           datadir=datadir, vocab_src_file=bpe_vocab_file, \n",
        "           vocab_trg_file=bpe_vocab_file, bpe_size=bpe_size)\n",
        "with open(\"transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIOosBx1fDIQ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D20-6ecg4PvC"
      },
      "source": [
        "This will take a while. The log reports the training process, look out for the prints of example translations and the BLEU evaluation scores to get an impression of the current quality. \n",
        "\n",
        "The log is also stored in the model directory within this runtime (inspect files in the menu on the left). There you can also find a summary report of all validations. We'll also use TensorBoard to visualize the training progress on the go. This requires enabling Cookies in the browser.\n",
        "\n",
        "After 12h at the latest, Colab will disconnect, so to make sure you're progress is not lost, download the checkpoints from the model directory from time to time. You'll later be able to reload them if model hyperparameters match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGzLfqzQEqq9",
        "outputId": "7606a757-14db-40c7-dc81-cc13cbccacaf"
      },
      "source": [
        "# Load the TensorBoard notebook extension. It will be empty at first.\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "9QYsZ513DuFi",
        "outputId": "a08061e9-75b7-481a-acfa-e066b013b86e"
      },
      "source": [
        "%tensorboard --logdir models/epo_eng_bpe4000_transformer/tensorboard "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6007, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF9do6ohedY6",
        "outputId": "fca665dd-8db5-439e-fda4-e612b425e23f"
      },
      "source": [
        "!python -m joeynmt train transformer_epo_eng_bpe4000.yaml"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-24 07:49:59,195 - INFO - root - Hello! This is Joey-NMT (version 1.2).\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "2021-02-24 07:49:59,253 - INFO - joeynmt.data - loading training data...\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
            "2021-02-24 07:50:04,524 - INFO - joeynmt.data - building vocabulary...\n",
            "2021-02-24 07:50:04,825 - INFO - joeynmt.data - loading dev data...\n",
            "2021-02-24 07:50:04,870 - INFO - joeynmt.data - loading test data...\n",
            "2021-02-24 07:50:04,877 - INFO - joeynmt.data - data loaded.\n",
            "2021-02-24 07:50:05.292394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-02-24 07:50:07,345 - INFO - joeynmt.training - Total params: 12223488\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.name                           : epo_eng_bpe4000_transformer\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.data.src                       : epo\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.data.trg                       : eng\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.data.train                     : /content/data/eng-epo//train.4000.bpe\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/data/eng-epo//dev.4000.bpe\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.data.test                      : /content/data/eng-epo//test.4000.bpe\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 30\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/data/eng-epo/joint.4000bpe.vocab\n",
            "2021-02-24 07:50:12,490 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/data/eng-epo/joint.4000bpe.vocab\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.remove_whitespace : True\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.tokenize     : intl\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-02-24 07:50:12,491 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/epo_eng_bpe4000_transformer\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-02-24 07:50:12,492 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-02-24 07:50:12,493 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 347669,\n",
            "\tvalid 1000,\n",
            "\ttest 1000\n",
            "2021-02-24 07:50:12,494 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] At@@ ing@@ ebla tabela ap@@ ud@@ skribo\n",
            "\t[TRG] Acces@@ sible T@@ able C@@ ap@@ tion\n",
            "2021-02-24 07:50:12,495 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) la (5) the (6) of (7) de (8) and (9) kaj\n",
            "2021-02-24 07:50:12,495 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) la (5) the (6) of (7) de (8) and (9) kaj\n",
            "2021-02-24 07:50:12,495 - INFO - joeynmt.helpers - Number of Src words (types): 4544\n",
            "2021-02-24 07:50:12,495 - INFO - joeynmt.helpers - Number of Trg words (types): 4544\n",
            "2021-02-24 07:50:12,495 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4544),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4544))\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "2021-02-24 07:50:12,499 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-02-24 07:50:12,500 - INFO - joeynmt.training - EPOCH 1\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "2021-02-24 07:50:39,092 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.188618, Tokens per Sec:    12128, Lr: 0.000300\n",
            "2021-02-24 07:51:05,360 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.467789, Tokens per Sec:    11703, Lr: 0.000300\n",
            "2021-02-24 07:51:32,445 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.635091, Tokens per Sec:    11792, Lr: 0.000300\n",
            "2021-02-24 07:51:59,372 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     5.116269, Tokens per Sec:    11885, Lr: 0.000300\n",
            "2021-02-24 07:52:26,509 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.292116, Tokens per Sec:    11908, Lr: 0.000300\n",
            "2021-02-24 07:52:53,698 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.685498, Tokens per Sec:    11679, Lr: 0.000300\n",
            "2021-02-24 07:53:20,573 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     3.585108, Tokens per Sec:    11482, Lr: 0.000300\n",
            "2021-02-24 07:53:47,534 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.488902, Tokens per Sec:    11808, Lr: 0.000300\n",
            "2021-02-24 07:54:14,494 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.433974, Tokens per Sec:    11811, Lr: 0.000300\n",
            "2021-02-24 07:54:41,498 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.492710, Tokens per Sec:    11746, Lr: 0.000300\n",
            "2021-02-24 07:55:18,095 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-02-24 07:55:18,095 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-02-24 07:55:18,555 - INFO - joeynmt.training - Example #0\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - \tSource:     Mi faris ion por vi.\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - \tReference:  I made you something.\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - \tHypothesis: I don't know you.\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - Example #1\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - \tSource:     Du oble du faras kvar.\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - \tReference:  Twice two is four.\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - \tHypothesis: - Thank you.\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - Example #2\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - \tSource:     Tio suferigas min! Ĉesu!\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - \tReference:  That hurts! Stop it!\n",
            "2021-02-24 07:55:18,556 - INFO - joeynmt.training - \tHypothesis: It's a cell!\n",
            "2021-02-24 07:55:18,557 - INFO - joeynmt.training - Example #3\n",
            "2021-02-24 07:55:18,557 - INFO - joeynmt.training - \tSource:     Estas tro multe por mi. Mi devas malrapidiĝi.\n",
            "2021-02-24 07:55:18,557 - INFO - joeynmt.training - \tReference:  It is too much for me. I need to slow down.\n",
            "2021-02-24 07:55:18,557 - INFO - joeynmt.training - \tHypothesis: It's not not not a boud to be a b.\n",
            "2021-02-24 07:55:18,557 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     2000: bleu:   1.58, loss: 48681.2578, ppl:  57.9049, duration: 37.0587s\n",
            "2021-02-24 07:55:24,844 - INFO - joeynmt.training - Epoch   1: total training loss 9282.08\n",
            "2021-02-24 07:55:24,844 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-02-24 07:55:46,369 - INFO - joeynmt.training - Epoch   2, Step:     2200, Batch Loss:     4.362589, Tokens per Sec:    11608, Lr: 0.000300\n",
            "2021-02-24 07:56:13,475 - INFO - joeynmt.training - Epoch   2, Step:     2400, Batch Loss:     3.232115, Tokens per Sec:    11551, Lr: 0.000300\n",
            "2021-02-24 07:56:40,605 - INFO - joeynmt.training - Epoch   2, Step:     2600, Batch Loss:     4.247138, Tokens per Sec:    11617, Lr: 0.000300\n",
            "2021-02-24 07:57:07,584 - INFO - joeynmt.training - Epoch   2, Step:     2800, Batch Loss:     2.437153, Tokens per Sec:    11502, Lr: 0.000300\n",
            "2021-02-24 07:57:34,622 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     3.080711, Tokens per Sec:    11618, Lr: 0.000300\n",
            "2021-02-24 07:58:01,490 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     2.964529, Tokens per Sec:    11500, Lr: 0.000300\n",
            "2021-02-24 07:58:28,496 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     3.148271, Tokens per Sec:    11919, Lr: 0.000300\n",
            "2021-02-24 07:58:55,475 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     2.473848, Tokens per Sec:    11824, Lr: 0.000300\n",
            "2021-02-24 07:59:22,488 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     3.504861, Tokens per Sec:    11791, Lr: 0.000300\n",
            "2021-02-24 07:59:49,383 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     2.078526, Tokens per Sec:    11680, Lr: 0.000300\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/__main__.py\", line 41, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/__main__.py\", line 29, in main\n",
            "    train(cfg_file=args.config_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/training.py\", line 748, in train\n",
            "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/training.py\", line 431, in train_and_validate\n",
            "    valid_duration = self._validate(valid_data, epoch_no)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/training.py\", line 521, in _validate\n",
            "    n_gpu=self.n_gpu\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/prediction.py\", line 120, in validate_on_data\n",
            "    beam_alpha=beam_alpha, max_output_length=max_output_length)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/search.py\", line 447, in run_batch\n",
            "    encoder_hidden=encoder_hidden)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/search.py\", line 39, in greedy\n",
            "    src_mask, max_output_length, model, encoder_output, encoder_hidden)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/search.py\", line 158, in transformer_greedy\n",
            "    if (finished >= 1).sum() == batch_size:\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgP9fnZsjO7K"
      },
      "source": [
        "## Continue training after interruption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj92bqnf8aOe"
      },
      "source": [
        "To continue after an interruption, the configuration needs to be modified in 2 places: \n",
        "1. `load_model` to point to the checkpoint to load.\n",
        "2. `model_dir` to create a new directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS7ixekMfG5F"
      },
      "source": [
        "ckpt_number = 8000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"models/{name}_transformer/1.ckpt\"', f'load_model: \"models/{name}_transformer/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/{name}_transformer\"', f'model_dir: \"models/{name}_transformer_continued\"')\n",
        "with open(\"transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05nDex2g9URU"
      },
      "source": [
        "Joey NMT then picks up training from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU63igoskRXJ",
        "outputId": "bc8cbf4b-2ee8-43cb-dc28-ebf32ddf3159"
      },
      "source": [
        "!python -m joeynmt train transformer_epo_eng_bpe4000_reload.yaml"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/__main__.py\", line 3, in <module>\n",
            "    from joeynmt.training import train\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/training.py\", line 18, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/__init__.py\", line 190, in <module>\n",
            "    from torch._C import *\n",
            "RuntimeError: KeyboardInterrupt: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Y-03KklJd3"
      },
      "source": [
        "## Let's Translate!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhR4-RFF5_dJ"
      },
      "source": [
        "The `test` mode can be used to translate (and evaluate on) the test set specified in the configuration. We usually do this only once after we've tuned hyperparameters on the dev set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu59HTo_lLJG",
        "outputId": "795ffa73-f211-4175-aae8-c982d7670fe8"
      },
      "source": [
        "!python -m joeynmt test models/epo_eng_bpe4000_transformer/config.yaml"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-24 08:00:19,593 - INFO - root - Hello! This is Joey-NMT (version 1.2).\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "2021-02-24 08:00:19,594 - INFO - joeynmt.data - building vocabulary...\n",
            "2021-02-24 08:00:19,897 - INFO - joeynmt.data - loading dev data...\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
            "2021-02-24 08:00:19,905 - INFO - joeynmt.data - loading test data...\n",
            "2021-02-24 08:00:19,912 - INFO - joeynmt.data - data loaded.\n",
            "2021-02-24 08:00:19,947 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
            "2021-02-24 08:00:23,099 - INFO - joeynmt.prediction - Decoding on dev set (/content/data/eng-epo//dev.4000.bpe.eng)...\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "2021-02-24 08:00:36,302 - INFO - joeynmt.prediction -  dev bleu[intl]:   1.97 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-02-24 08:00:36,302 - INFO - joeynmt.prediction - Decoding on test set (/content/data/eng-epo//test.4000.bpe.eng)...\n",
            "2021-02-24 08:00:49,635 - INFO - joeynmt.prediction - test bleu[intl]:   1.99 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RgkT7mp6T2l"
      },
      "source": [
        "The `translate` mode is more interactive and takes prompts to translate interactively. Warning: it requires applying the same pre-processing steps to the new input as you've applied before model training (i.e. splitting into subwords)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpBTLaoM9g58"
      },
      "source": [
        "from subword_nmt import apply_bpe\n",
        "\n",
        "with open(bpe_file, \"r\") as merge_file:\n",
        "  bpe = apply_bpe.BPE(codes=merge_file)\n",
        "\n",
        "preprocess = lambda x: bpe.process_line(x.strip())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4clB81W46oTB"
      },
      "source": [
        "my_sentence = 'Esperanto, origine la Lingvo Internacia, estas la plej disvastiĝinta internacia planlingvo.'   # From https://eo.wikipedia.org/wiki/Esperanto"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OUkf54ip-hVw",
        "outputId": "00fe1ee1-dfca-4d0c-ac82-a4fa33829acd"
      },
      "source": [
        "preprocess(my_sentence)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'E@@ sper@@ ant@@ o, or@@ ig@@ ine la L@@ ingv@@ o In@@ tern@@ aci@@ a, estas la plej dis@@ v@@ ast@@ iĝ@@ inta intern@@ ac@@ ia plan@@ lingv@@ o.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtS7xyEmmD90",
        "outputId": "4874b59d-a3e9-423e-de08-8617edea3ea7"
      },
      "source": [
        "!python -m joeynmt translate models/epo_eng_bpe4000_transformer/config.yaml"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-24 08:00:51,240 - INFO - root - Hello! This is Joey-NMT (version 1.2).\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "E@@ sper@@ ant@@ o, or@@ ig@@ ine la L@@ ingv@@ o In@@ tern@@ aci@@ a, estas la plej dis@@ v@@ ast@@ iĝ@@ inta intern@@ ac@@ ia plan@@ lingv@@ o.\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "JoeyNMT: Ever, we have to the Lule of the Taper of the Toug.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "\n",
            "Bye.\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}